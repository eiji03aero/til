<!--
{
  "type": "summary",
  "tags": ["database"]
}
-->
# Isolation levels and locking in relational databases
https://retool.com/blog/isolation-levels-and-locking-in-relational-databases/

---

# Summary
- modern rdbms needs a mechanism to save the integrity of data.
- basically they have what's so called locks and isolation

# Content
## Why you need database locking and isolation
- as business grows, system needs to handle more request. for database this means dealing with concurrent transactions
- things go harder when dealing with concurrent transactions, because problems like following three would occur

## Big 3
- the three types of problems that corrupt data integrity for user
- dirty read
  - t1 reads and update data
  - t2 reads before t1 update
  - what t2 read is stale
- non repeatable read
  - t1 reads and reads one more time later
  - t2 update data before t1 reads second time
  - what t1 reads second time is different from the first
- phantom read
  - t1 reads and reads one more time later
  - t2 update data to add some new data before t1 reads second time
  - what t1 reads second time has something new/different than the first

## How locking works
- when transaction acquires a "lock", other transaction cannot operate on the data that is locked by it
- level of lock are:
  - row (common)
  - page
  - file
  - table (common)
  - database (eg. system update, command utility)

## level of isolations
- READ_UNCOMMITTED
  - allow reading even if the data is under lock for this transaction
- READ_COMMITTED
  - if the data is under lock, wait to read until it is committed (commit or rollback)
- REPATABLE_READ
  - it locks all the used data for the entire transaction
- SERIALIZABLE
  - the most intence isolation
  - it basically looks, when dealing with concurrent transactions, they are operated sequentially.
  - concurrent operation is taken only when end result is expected to be same

## problems derived from locking
- lock contention
  - flow:
    - t1 reads and update row_a
    - t2_reads and update row_a
    - t3 reads and update row_a
    - t4 just reads
  - if certain data keeps on being acquired locks, other requests will always have to wait for the previous locks to be done.
- long term blocking
  - if one transaction takes long time, like updating all rows of huge table, other transactions cannot operate on this table
- deadlock
  - flow:
    - t1 reads row_a
    - t2 reads row_b
    - t1 tries to read row_b
    - t2 tries to read row_a
  - two transactions will have to wait for lock to be resolved, but this won't happen
  - usually rdbms tries to solve this by rolling back whichever transaction is easy to rollback

# Thoughts
- how is phantom-read really different from non repeatable read?
- really didn't understand level of isolations from this article. should read something else
